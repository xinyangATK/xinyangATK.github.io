---
---

@article{li2023patchct,
      title={PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification}, 
      author={Li*, Miaoge and Wang*, Dongsheng and Liu, Xinyang, and  Zeng, Zequn and Lu, Ruiying and Chen, Bo and Zhou, Mingyuan},
      journal={The IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)},
      year={2023},
      preview={PatchCT.png},
      abstract={Multi-label image classification is a prediction task that aims to identify more than one label from a given image. This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap. While recent cross-modal attention-based studies have attempted to align such two representations and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation. We find that by formulating the multi-label classification as a CT problem, we can exploit the interactions between the image and label efficiently by minimizing the bidirectional CT cost. Specifically, after feeding the images and textual labels into the modality-specific encoders, we view each image as a mixture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class prototypes, respectively. CT is then employed to learn and align those two semantic sets by defining the forward and backward navigators. Importantly, the defined navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes. Extensive experiments on three public image benchmarks show that the proposed model consistently outperforms the previous methods.},
      arxiv={2307.09066},
      code={https://github.com/keepgoingjkg/PatchCT},
}

@article{liu2023patch,
  title={Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models},
  author={Liu*, Xinyang and Wang*, Dongsheng and Li, Miaoge and Duan, Zhibin and Xu, Yishi and Chen, Bo and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2303.09100},
  year={2023},
  preview={PBPrompt.png},
  abstract={For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwardly extended to the conditional case where the instance-conditional prompts are generated to improve the generalizability. Extensive experiments on 15 datasets show promising transferability and generalization performance of our proposed model.},
  arxiv={2303.09100},
  selected={true},
}

@article{liu2023patch,
  title={Bayesian Progressive Deep Topic Model with Knowledge Informed Textual Data Coarsening Process},
  author={Duan*, Zhibin and Liu*, Xinyang and Su, Yudi and Xu, Yishi and Chen, Bo and Zhou, Mingyuan},
  journal={In the 40th International Conference on Machine Learning (<strong>ICML</strong>)},
  year={2023},
  preview={ProGBN.png},
  abstract={Deep topic models have shown an impressive abil- ity to extract multi-layer document latent repre- sentations and discover hierarchical semantically meaningful topics. However, most deep topic models are limited to the single-step generative process, despite the fact that the progressive gen- erative process has achieved impressive perfor- mance in modeling image data. To this end, in this paper, we propose a novel progressive deep topic model that consists of a knowledge-informed tex- tural data coarsening process and a correspond- ing progressive generative model. The former is used to build multi-level observations ranging from concrete to abstract, while the latter is used to generate more concrete observations gradually. Additionally, we incorporate a graph-enhanced de- coder to capture the semantic relationships among words at different levels of observation. Further- more, we perform a theoretical analysis of the proposed model based on the principle of informa- tion theory and show how it can alleviate the well- known “latent variable collapse” problem. Finally, extensive experiments demonstrate that our pro- posed model effectively improves the ability of deep topic models, resulting in higher-quality la- tent document representations and topics.},
  selected={true},
}




