<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Xinyang Liu </title> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://xinyangatk.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> </h1> <p class="desc"></p> </header> <article> <div class="profile float-left"> <figure> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <h1 id="xinyang-liu-ÂàòÊòïÊ¥ã"><strong>Xinyang Liu (ÂàòÊòïÊ¥ã)</strong></h1> <p><span style="color: black; font-size: 1.1em;">Statistics and Data Sciences (SDS)</span><br> <span style="color: #BF5701; font-size: 1.2em; font-weight: bold;">The University of Texas at Austin</span></p> <p><span style="color: Teal; font-size: 1.0em; font-weight: bold;">Email: xinyangATK [AT] gmail [dot] com</span><br> <strong><a href="https://scholar.google.com.hk/citations?hl=zh-CN&amp;user=9VtswyYAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> | <a href="https://github.com/xinyangATK" rel="external nofollow noopener" target="_blank">Github</a> | <a href="https://twitter.com/XinyangATK" rel="external nofollow noopener" target="_blank">Twitter</a></strong></p> <h2 id="bio"><strong>Bio</strong></h2> <p>Howdy! I am the first-year PhD student at The University of Texas at Austin <img src="/assets/img/ut-logo.png" width="65">, advised by <strong><a href="https://mingyuanzhou.github.io" rel="external nofollow noopener" target="_blank"><u>Prof. Mingyuan Zhou</u></a></strong>. I received my M.S degree from Xidian University in 2024, advised by <strong><a href="https://web.xidian.edu.cn/bchen/" rel="external nofollow noopener" target="_blank"><u>Prof. Bo Chen</u></a></strong>. Previously, I obtained my B.S degree from Xidian University in 2021. <br> <br> <br> <strong>My research interests</strong> lie in the general area of probablistic modeling, particularly in solving real-world problems through advanced Generative AI systems. My recent research focuses on <strong>Generative Modeling</strong>, including its <strong><u>theory exploration and various applications in data generation (e.g., dLLMs) and multimodal learning.</u></strong> I am currently exploring <strong>all dimensions of diffusion Large Language Models (dLLMs)</strong>, including foundation-model design, inference acceleration, and post-training for downstream tasks.</p> <p>If you share the same research interests with me, feel free to reach out or add my <a href="./assets/img/wechat.jpg">WeChat</a>.</p> <p>I‚Äôm actively looking for <strong style="color: red;">internship opportunities starting in Summer 2026</strong>. Feel free to reach out if you‚Äôre interested in my research.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Oct, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2509.25035v2" rel="external nofollow noopener" target="_blank">‚ÄúUltra-Fast Language Generation via Discrete Diffusion Divergence Instruct‚Äù</a> is accepted by ICLR 2026 üáßüá∑‚öΩÔ∏èüèñÔ∏è! Welcome to explore the <a href="https://haoyangzheng.github.io/research/didi-instruct/" rel="external nofollow noopener" target="_blank">Ultra-Fast Language Generation</a> further for more details! </td> </tr> <tr> <th scope="row">Jun, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2503.01776" rel="external nofollow noopener" target="_blank">‚ÄúBeyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation‚Äù</a> has been selected for an <strong style="color: red;">Oral Presentation</strong> at <strong>ICML 2025</strong>! Big congratulations to <a href="https://neilwen987.github.io" rel="external nofollow noopener" target="_blank">Tiansheng</a>! </td> </tr> <tr> <th scope="row">Mar, 2025</th> <td> I‚Äôm thrilled to accept the offer from UT-Austin and can‚Äôt wait to enjoy the legendary barbecue üî•üçñü§† in Austin! </td> </tr> <tr> <th scope="row">Jan, 2025</th> <td> <span style="font-family: 'Bradley Hand', cursive; color: #FF5809; font-weight: 900;">Happy Birthday</span>üç∞üïØÔ∏èüëë The best gifts come from two accepted papers! Much appreciation to all of my collaborators and advisors! <br> In <a href="https://xinyangatk.github.io" rel="external nofollow noopener" target="_blank">‚ÄúOptimal Stochastic Trace Estimation in Generative Modeling‚Äù</a> (<strong>AISTATS 2025</strong>), we leverage the Hutch++ estimator in generative modeling and propose a practical algorithm that amortizes decompositions to reduce costs, while also providing theoretical guarantees specifically in generative modeling context. <br> In <a href="http://arxiv.org/abs/2406.09357" rel="external nofollow noopener" target="_blank">‚ÄúAdvancing Graph Generation through Beta Diffusion‚Äù</a> (<strong>ICLR 2025</strong>), we futher explore the potential of Beta Diffusion in graph modeling and propose a novel graph-driven generative process with concentration modulation technique, which makes Beta Diffusion unique again! </td> </tr> <tr> <th scope="row">Jun, 2024</th> <td> I graduated with a master‚Äôs degree in Xidian University! </td> </tr> <tr> <th scope="row">Apr, 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2303.09100" rel="external nofollow noopener" target="_blank">‚ÄúPatch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models‚Äù</a> is accepted by <strong>UAI 2024</strong>. </td> </tr> <tr> <th scope="row">Sep, 2023</th> <td> Two papers are accepted by <strong>NeurIPS 2023</strong>! </td> </tr> <tr> <th scope="row">Apr, 2023</th> <td> Our paper <a href="https://proceedings.mlr.press/v202/duan23c.html" rel="external nofollow noopener" target="_blank">‚ÄúBayesian Progressive Deep Topic Model with Knowledge Informed Textual Data Coarsening Process‚Äù</a> is accepted by <strong>ICML 2023</strong>. </td> </tr> <tr> <th scope="row">Feb, 2023</th> <td> <img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"><img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"><img class="emoji" title=":fire:" alt=":fire:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png" height="20" width="20"> We have open sourced a new version of <strong><a href="https://github.com/BoChenGroup/PyDPM" rel="external nofollow noopener" target="_blank">PyDPM</a></strong>, and welcome to join the open source library of deep probabilistic models! <br> <strong>PyDPM</strong> is a python library focuses on constructing <strong>D</strong>eep <strong>P</strong>robabilistic <strong>M</strong>odels (<strong>DPMs</strong>). Our developed Pydpm not only provides efficient distribution sampling functions on GPU, but also has included the implementations of existing popular DPMs. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <p>(*) denotes equal contribution</p> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/DiDi-Instruct.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="DiDi-Instruct.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zheng2025didiinstruct" class="col-sm-8"> <div class="title">Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct</div> <div class="author"> <a href="https://haoyangzheng.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Haoyang Zheng</a>,¬†<em><strong style="color: #B509AC;">Xinyang Liu</strong></em>,¬†Xiangrui Kong,¬†<a href="https://jiangnanhugo.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Nan Jiang</a>,¬†<a href="https://scholar.google.com/citations?user=On2YFigAAAAJ&amp;hl=zh-CN" style="color: black;" rel="external nofollow noopener" target="_blank">Zheyuan Hu</a>,¬†<a href="https://pkulwj1994.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Weijian Luo</a>,¬†<a href="https://www.weideng.org/" style="color: black;" rel="external nofollow noopener" target="_blank">Wei Deng</a>,¬†and¬†<a href="https://www.math.purdue.edu/~lin491/" style="color: black;" rel="external nofollow noopener" target="_blank">Guang Lin</a> </div> <div class="periodical"> <em>The Fourteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, <em>2026</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2509.25035" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://haoyangzheng.github.io/research/didi-instruct" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Fast generation of language texts is the holy grail that people pursue in the AI era. In this work, we introduced Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that leads to fast language generation models by initializing from a pre-trained (masked) discrete diffusion language model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical part of the paper, we build the foundation of DiDi-Instruct in a framework of integral KL-divergence minimization, with practical training algorithms. We also introduce techniques like grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler (RGAS) that significantly improve the training stability, the model coverage, and the inference performances. On OpenWebText, DiDi-Instruct outperforms all accelerated language generation models as well as the GPT-2 baseline and the standard dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128 NFEs). These performance gains are accomplished with a negligible entropy loss of about 1% and 20x less additional training wall-clock time. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at this http URL.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/CSR.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="CSR.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2025csr" class="col-sm-8"> <div class="title">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</div> <div class="author"> <a href="https://neilwen987.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Tiansheng Wen*</a>,¬†<a href="https://yifeiwang77.com" style="color: black;" rel="external nofollow noopener" target="_blank">Yifei Wang*</a>,¬†zequn Zeng,¬†Zhong Peng,¬†Yudi Su,¬†<em><strong style="color: #B509AC;">Xinyang Liu</strong></em>,¬†<a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>,¬†Hongwei Liu,¬†<a href="https://people.csail.mit.edu/stefje/" style="color: black;" rel="external nofollow noopener" target="_blank">Stefanie Jegelka</a>,¬†and¬†<a href="https://chenyuyou.me" style="color: black;" rel="external nofollow noopener" target="_blank">Chenyu You</a> </div> <div class="periodical"> <em>Forty-Second International Conference on Machine Learning (<strong>ICML</strong>)</em>, <em>2025</em> </div> <div class="periodical"> <strong style="color: red;">Oral Presentation [Top¬†1%]</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2503.01776" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/neilwen987/CSR_Adaptive_Rep" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/hutch++.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="hutch++.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024hutch++" class="col-sm-8"> <div class="title">Optimal Stochastic Trace Estimation in Generative Modeling</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>,¬†<a href="https://hengrongdu.netlify.app/" style="color: black;" rel="external nofollow noopener" target="_blank">Hengrong Du*</a>,¬†<a href="https://www.weideng.org/" style="color: black;" rel="external nofollow noopener" target="_blank">Wei Deng</a>,¬†and¬†<a href="https://ruqizhang.github.io/" style="color: black;" rel="external nofollow noopener" target="_blank">Ruqi Zhang</a> </div> <div class="periodical"> <em>The 28th International Conference on Artificial Intelligence and Statistics (<strong>AISTATS</strong>)</em>, <em>2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2502.18808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/GenHutch-plus-plus" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Hutchinson estimators are widely employed in training divergence-based likelihoods for diffusion models to ensure optimal transport (OT) properties. However, this estimator often suffers from high variance and scalability concerns. To address these challenges, we investigate Hutch++, an optimal stochastic trace estimator for generative models, designed to minimize training variance while maintaining transport optimality. Hutch++ is particularly effective for handling ill-conditioned matrices with large condition numbers, which commonly arise when high-dimensional data exhibits a low-dimensional structure. To mitigate the need for frequent and costly QR decompositions, we propose practical schemes that balance frequency and accuracy, backed by theoretical guarantees. Our analysis demonstrates that Hutch++ leads to generations of higher quality. Furthermore, this method exhibits effective variance reduction in various applications, including simulations, conditional time series forecasts, and image generation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/GBD.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="GBD.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="he2024betagraph" class="col-sm-8"> <div class="title">Advancing Graph Generation through Beta Diffusion</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>,¬†Yilin He*,¬†<a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>,¬†and¬†<a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>The Thirteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, <em>2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2406.09357" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/GraphBetaDiffusion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Diffusion models have demonstrated effectiveness for generating natural images and have since been adapted to generate diverse types of data, including graphs. While this emerging family of diffusion-based graph generative models has shown remarkable performance gains over predecessors that rely on variational autoencoders or generative adversarial networks, it is important to note that the majority of these models utilize Gaussian or categorical-based diffusion processes, which may encounter difficulties when modeling sparse and long-tailed data distributions. In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based generative model adept at modeling diverse graph structures. Focusing on the sparse and range-bounded characteristics of graph adjacency matrices, GBD employs a beta diffusion process to ensure that the initial distribution aligns with the beta distribution, which is well-suited for modeling such data types. To enhance the realism of generated graphs further, we introduce a modulation technique that stabilizes the generation of important graph structures while maintaining flexibility for the rest. The superior performance of GBD in generating graphs, as demonstrated across three generic graph benchmarks and two biochemical graph benchmarks, underscores its effectiveness in capturing the complexities of real-world graph data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/PBPrompt.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="PBPrompt.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024patch" class="col-sm-8"> <div class="title">Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>,¬†Dongsheng Wang*,¬†Bowei Fang,¬†<a href="https://keepgoingjkg.github.io/about/" style="color: black;" rel="external nofollow noopener" target="_blank">Miaoge Li</a>,¬†<a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=bITyHaEAAAAJ" style="color: black;" rel="external nofollow noopener" target="_blank">Zhibin Duan</a>,¬†Yishi Xu,¬†<a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>,¬†and¬†<a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>Proceedings of the 40th Conference on Uncertainty in Artificial Intelligence (<strong>UAI</strong>)</em>, <em>2024</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2303.09100" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/PBPrompt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt tuning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize the tuning process by minimizing the statistical distance between the visual patches and linguistic prompts, which pushes the stochastic label representations to faithfully capture diverse visual concepts, instead of overfitting the training categories. We evaluate the effectiveness of our approach on four tasks: few-shot image recognition, base-to-new generalization, dataset transfer learning, and domain shifts. Extensive results over 15 datasets show promising transferability and generalization performance of our proposed model, both quantitatively and qualitatively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/ProGBN.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ProGBN.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mlr-v202-duan23c" class="col-sm-8"> <div class="title">Bayesian Progressive Deep Topic Model with Knowledge Informed Textual Data Coarsening Process</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=bITyHaEAAAAJ" style="color: black;" rel="external nofollow noopener" target="_blank">Zhibin Duan*</a>,¬†<em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>,¬†Yudi Su,¬†Yishi Xu,¬†<a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>,¬†and¬†<a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>The 40th International Conference on Machine Learning (<strong>ICML</strong>)</em>, <em>2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://proceedings.mlr.press/v202/duan23c.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/ProGBN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Deep topic models have shown an impressive ability to extract multi-layer document latent representations and discover hierarchical semantically meaningful topics. However, most deep topic models are limited to the single-step generative process, despite the fact that the progressive generative process has achieved impressive performance in modeling image data. To this end, in this paper, we propose a novel progressive deep topic model that consists of a knowledge-informed textural data coarsening process and a corresponding progressive generative model. The former is used to build multi-level observations ranging from concrete to abstract, while the latter is used to generate more concrete observations gradually. Additionally, we incorporate a graph-enhanced decoder to capture the semantic relationships among words at different levels of observation. Furthermore, we perform a theoretical analysis of the proposed model based on the principle of information theory and show how it can alleviate the wellknown ‚Äúlatent variable collapse‚Äù problem. Finally, extensive experiments demonstrate that our proposed model effectively improves the ability of deep topic models, resulting in higher-quality latent document representations and topics.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2026 Xinyang Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>