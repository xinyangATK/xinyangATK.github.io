<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Xinyang Liu | Publications </title> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://xinyangatk.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Xinyang </span>Liu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">(*) denotes equal contribution</p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/DiDi-Instruct.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="DiDi-Instruct.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zheng2025didiinstruct" class="col-sm-8"> <div class="title">Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct</div> <div class="author"> <a href="https://haoyangzheng.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Haoyang Zheng</a>, <em><strong style="color: #B509AC;">Xinyang Liu</strong></em>, Xiangrui Kong, <a href="https://jiangnanhugo.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Nan Jiang</a>, <a href="https://scholar.google.com/citations?user=On2YFigAAAAJ&amp;hl=zh-CN" style="color: black;" rel="external nofollow noopener" target="_blank">Zheyuan Hu</a>, <a href="https://pkulwj1994.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Weijian Luo</a>, <a href="https://www.weideng.org/" style="color: black;" rel="external nofollow noopener" target="_blank">Wei Deng</a>, and <a href="https://www.math.purdue.edu/~lin491/" style="color: black;" rel="external nofollow noopener" target="_blank">Guang Lin</a> </div> <div class="periodical"> <em>Arxiv 2509.25035</em>, <em>2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2509.25035" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://haoyangzheng.github.io/research/didi-instruct" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Fast generation of language texts is the holy grail that people pursue in the AI era. In this work, we introduced Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that leads to fast language generation models by initializing from a pre-trained (masked) discrete diffusion language model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical part of the paper, we build the foundation of DiDi-Instruct in a framework of integral KL-divergence minimization, with practical training algorithms. We also introduce techniques like grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler (RGAS) that significantly improve the training stability, the model coverage, and the inference performances. On OpenWebText, DiDi-Instruct outperforms all accelerated language generation models as well as the GPT-2 baseline and the standard dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128 NFEs). These performance gains are accomplished with a negligible entropy loss of about 1% and 20x less additional training wall-clock time. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at this http URL.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/CSR.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="CSR.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2025csr" class="col-sm-8"> <div class="title">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</div> <div class="author"> <a href="https://neilwen987.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Tiansheng Wen*</a>, <a href="https://yifeiwang77.com" style="color: black;" rel="external nofollow noopener" target="_blank">Yifei Wang*</a>, zequn Zeng, Zhong Peng, Yudi Su, <em><strong style="color: #B509AC;">Xinyang Liu</strong></em>, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, Hongwei Liu, <a href="https://people.csail.mit.edu/stefje/" style="color: black;" rel="external nofollow noopener" target="_blank">Stefanie Jegelka</a>, and <a href="https://chenyuyou.me" style="color: black;" rel="external nofollow noopener" target="_blank">Chenyu You</a> </div> <div class="periodical"> <em>Forty-Second International Conference on Machine Learning (<strong>ICML</strong>)</em>, <em>2025</em> </div> <div class="periodical"> <strong style="color: red;">Oral Presentation [Top 1%]</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2503.01776" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/neilwen987/CSR_Adaptive_Rep" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/hutch++.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="hutch++.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024hutch++" class="col-sm-8"> <div class="title">Optimal Stochastic Trace Estimation in Generative Modeling</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>, <a href="https://hengrongdu.netlify.app/" style="color: black;" rel="external nofollow noopener" target="_blank">Hengrong Du*</a>, <a href="https://www.weideng.org/" style="color: black;" rel="external nofollow noopener" target="_blank">Wei Deng</a>, and <a href="https://ruqizhang.github.io/" style="color: black;" rel="external nofollow noopener" target="_blank">Ruqi Zhang</a> </div> <div class="periodical"> <em>The 28th International Conference on Artificial Intelligence and Statistics (<strong>AISTATS</strong>)</em>, <em>2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2502.18808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/GenHutch-plus-plus" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Hutchinson estimators are widely employed in training divergence-based likelihoods for diffusion models to ensure optimal transport (OT) properties. However, this estimator often suffers from high variance and scalability concerns. To address these challenges, we investigate Hutch++, an optimal stochastic trace estimator for generative models, designed to minimize training variance while maintaining transport optimality. Hutch++ is particularly effective for handling ill-conditioned matrices with large condition numbers, which commonly arise when high-dimensional data exhibits a low-dimensional structure. To mitigate the need for frequent and costly QR decompositions, we propose practical schemes that balance frequency and accuracy, backed by theoretical guarantees. Our analysis demonstrates that Hutch++ leads to generations of higher quality. Furthermore, this method exhibits effective variance reduction in various applications, including simulations, conditional time series forecasts, and image generation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/GBD.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="GBD.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="he2024betagraph" class="col-sm-8"> <div class="title">Advancing Graph Generation through Beta Diffusion</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>, Yilin He*, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>The Thirteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, <em>2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2406.09357" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/GraphBetaDiffusion" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Diffusion models have demonstrated effectiveness for generating natural images and have since been adapted to generate diverse types of data, including graphs. While this emerging family of diffusion-based graph generative models has shown remarkable performance gains over predecessors that rely on variational autoencoders or generative adversarial networks, it is important to note that the majority of these models utilize Gaussian or categorical-based diffusion processes, which may encounter difficulties when modeling sparse and long-tailed data distributions. In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based generative model adept at modeling diverse graph structures. Focusing on the sparse and range-bounded characteristics of graph adjacency matrices, GBD employs a beta diffusion process to ensure that the initial distribution aligns with the beta distribution, which is well-suited for modeling such data types. To enhance the realism of generated graphs further, we introduce a modulation technique that stabilizes the generation of important graph structures while maintaining flexibility for the rest. The superior performance of GBD in generating graphs, as demonstrated across three generic graph benchmarks and two biochemical graph benchmarks, underscores its effectiveness in capturing the complexities of real-world graph data.</p> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/PBPrompt.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="PBPrompt.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024patch" class="col-sm-8"> <div class="title">Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models</div> <div class="author"> <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>, Dongsheng Wang*, Bowei Fang, <a href="https://keepgoingjkg.github.io/about/" style="color: black;" rel="external nofollow noopener" target="_blank">Miaoge Li</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=bITyHaEAAAAJ" style="color: black;" rel="external nofollow noopener" target="_blank">Zhibin Duan</a>, Yishi Xu, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>Proceedings of the 40th Conference on Uncertainty in Artificial Intelligence (<strong>UAI</strong>)</em>, <em>2024</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2303.09100" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/PBPrompt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt tuning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize the tuning process by minimizing the statistical distance between the visual patches and linguistic prompts, which pushes the stochastic label representations to faithfully capture diverse visual concepts, instead of overfitting the training categories. We evaluate the effectiveness of our approach on four tasks: few-shot image recognition, base-to-new generalization, dataset transfer learning, and domain shifts. Extensive results over 15 datasets show promising transferability and generalization performance of our proposed model, both quantitatively and qualitatively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/WGAAE.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="WGAAE.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024scalable" class="col-sm-8"> <div class="title">Scalable Weibull Graph Attention Autoencoder for Modeling Document Networks</div> <div class="author"> Chaojie Wang*, <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>, Dongsheng Wang, Hao Zhang, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>ArXiv 2410.09696, 2024</em>, <em>2024</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2410.09696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Although existing variational graph autoencoders (VGAEs) have been widely used for modeling and generating graph-structured data, most of them are still not flexible enough to approximate the sparse and skewed latent node representations, especially those of document relational networks (DRNs) with discrete observations. To analyze a collection of interconnected documents, a typical branch of Bayesian models, specifically relational topic models (RTMs), has proven their efficacy in describing both link structures and document contents of DRNs, which motives us to incorporate RTMs with existing VGAEs to alleviate their potential issues when modeling the generation of DRNs. In this paper, moving beyond the sophisticated approximate assumptions of traditional RTMs, we develop a graph Poisson factor analysis (GPFA), which provides analytic conditional posteriors to improve the inference accuracy, and extend GPFA to a multi-stochastic-layer version named graph Poisson gamma belief network (GPGBN) to capture the hierarchical document relationships at multiple semantic levels. Then, taking GPGBN as the decoder, we combine it with various Weibull-based graph inference networks, resulting in two variants of Weibull graph auto-encoder (WGAE), equipped with model inference algorithms. Experimental results demonstrate that our models can extract high-quality hierarchical latent document representations and achieve promising performance on various graph analytic tasks.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Meta_CETM.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Meta_CETM.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023metacetm" class="col-sm-8"> <div class="title">Context-guided Embedding Adaptation for Effective Topic Modeling in Low-Resource Regimes</div> <div class="author"> Yishi Xu, Jianqiao Sun, Yudi Su, <em><strong style="color: #B509AC;">Xinyang Liu</strong></em>, <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=bITyHaEAAAAJ" style="color: black;" rel="external nofollow noopener" target="_blank">Zhibin Duan</a>, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>The 37th Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, <em>2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Embedding-based neural topic models have been shown as superior options for few-shot topic modeling. However, existing approaches treat the static word embeddings learned from source tasks as transferable knowledge which can be directly applied to the target task, ignoring the fact that word meanings can vary across tasks with different contexts, thus leading to suboptimal results when adapting to new tasks with novel contexts. To address the issue, in this paper, we propose an effective approach for topic modeling under the low-resource regime, the core of which is the adaptive generation of semantic matching word embeddings by integrating the contextual information of each task. Concretely, we introduce a variational graph autoencoder to learn task-specific word embeddings based on the dependency graph refined from the context of each task, with a learnable Gaussian mixture prior to capture the clustering structure of distributed word representations. This is naturally connected to topic modeling by regarding each component of the mixture as the representation of a topic, which facilitates the discovery of diverse topics and the fast adaptation to novel tasks. Both quantitative and qualitative experiments demonstrate the superiority of our method against established topic models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/ALIGN.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ALIGN.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023align" class="col-sm-8"> <div class="title">Tuning Multi-mode Token-level Prompt Alignment across Modalities</div> <div class="author"> Dongsheng Wang, <a href="https://keepgoingjkg.github.io/about/" style="color: black;" rel="external nofollow noopener" target="_blank">Miaoge Li</a>, <em><strong style="color: #B509AC;">Xinyang Liu</strong></em>, Xu Mingsheng, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and Zhang Hanwang</div> <div class="periodical"> <em>The 37th Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, <em>2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Prompt tuning pre-trained vision-language models have demonstrated significant potential in improving open-world visual concept understanding. However, prior works only primarily focus on single-mode (only one prompt for each modality) and holistic level (image or sentence) semantic alignment, which fails to capture the sample diversity, leading to sub-optimal prompt discovery. To address the limitation, we propose a multi-mode token-level tuning framework that leverages the optimal transportation to learn and align a set of prompt tokens across modalities. Specifically, we rely on two essential factors: 1) multi-mode prompts discovery, which guarantees diverse semantic representations, and 2) token-level alignment, which helps explore fine-grained similarity. Thus, the similarity can be calculated as a hierarchical transportation problem between the modality-specific sets. Extensive experiments on popular image recognition benchmarks show the superior generalization and few-shot abilities of our approach. The qualitative analysis demonstrates that the learned prompt tokens have the ability to capture diverse visual concepts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/PatchCT.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="PatchCT.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023patchct" class="col-sm-8"> <div class="title">PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification</div> <div class="author"> <a href="https://keepgoingjkg.github.io/about/" style="color: black;" rel="external nofollow noopener" target="_blank">Miaoge Li*</a>, Dongsheng Wang*, <em><strong style="color: #B509AC;">Xinyang Liu</strong></em>, <a href="https://joeyz0z.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Zequn Zeng</a>, <a href="https://web.xidian.edu.cn/luruiying/" style="color: black;" rel="external nofollow noopener" target="_blank">Ruiying Lu</a>, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, <em>2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="http://arxiv.org/abs/2307.09066" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/keepgoingjkg/PatchCT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Multi-label image classification is a prediction task that aims to identify more than one label from a given image. This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap. While recent cross-modal attention-based studies have attempted to align such two representations and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation. We find that by formulating the multi-label classification as a CT problem, we can exploit the interactions between the image and label efficiently by minimizing the bidirectional CT cost. Specifically, after feeding the images and textual labels into the modality-specific encoders, we view each image as a mixture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class prototypes, respectively. CT is then employed to learn and align those two semantic sets by defining the forward and backward navigators. Importantly, the defined navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes. Extensive experiments on three public image benchmarks show that the proposed model consistently outperforms the previous methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 preview"> <figure> <picture> <img src="/assets/img/publication_preview/ProGBN.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ProGBN.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mlr-v202-duan23c" class="col-sm-8"> <div class="title">Bayesian Progressive Deep Topic Model with Knowledge Informed Textual Data Coarsening Process</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=bITyHaEAAAAJ" style="color: black;" rel="external nofollow noopener" target="_blank">Zhibin Duan*</a>, <em><strong style="color: #B509AC;">Xinyang Liu*</strong></em>, Yudi Su, Yishi Xu, <a href="https://web.xidian.edu.cn/bchen" style="color: black;" rel="external nofollow noopener" target="_blank">Bo Chen</a>, and <a href="https://mingyuanzhou.github.io" style="color: black;" rel="external nofollow noopener" target="_blank">Mingyuan Zhou</a> </div> <div class="periodical"> <em>The 40th International Conference on Machine Learning (<strong>ICML</strong>)</em>, <em>2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">ABSTRACT</a> <a href="https://proceedings.mlr.press/v202/duan23c.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xinyangATK/ProGBN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Deep topic models have shown an impressive ability to extract multi-layer document latent representations and discover hierarchical semantically meaningful topics. However, most deep topic models are limited to the single-step generative process, despite the fact that the progressive generative process has achieved impressive performance in modeling image data. To this end, in this paper, we propose a novel progressive deep topic model that consists of a knowledge-informed textural data coarsening process and a corresponding progressive generative model. The former is used to build multi-level observations ranging from concrete to abstract, while the latter is used to generate more concrete observations gradually. Additionally, we incorporate a graph-enhanced decoder to capture the semantic relationships among words at different levels of observation. Furthermore, we perform a theoretical analysis of the proposed model based on the principle of information theory and show how it can alleviate the wellknown “latent variable collapse” problem. Finally, extensive experiments demonstrate that our proposed model effectively improves the ability of deep topic models, resulting in higher-quality latent document representations and topics.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Xinyang Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>